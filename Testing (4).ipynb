{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rUqj0nbcz_lX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 06:44:11.502260: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-29 06:44:11.516511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761720251.533903  102393 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761720251.539497  102393 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761720251.552808  102393 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761720251.552824  102393 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761720251.552827  102393 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761720251.552828  102393 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-29 06:44:11.556916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth on – no per-process hard cap.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, random, glob\n",
    "# ─── 1. Env-vars BEFORE TF is imported ─────────────────────────────\n",
    "os.environ.pop(\"TF_GPU_ALLOCATOR\", None)  \n",
    "#os.environ[\"TF_GPU_ALLOCATOR\"]      = \"cuda_malloc_async\"\n",
    "#os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "#os.environ[\"TF_XLA_FLAGS\"]          = \"--tf_xla_auto_jit=0\"\n",
    "\n",
    "import tensorflow as tf             # ← TF sees the env-vars above\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # 1.  ***DO NOT*** call VirtualDeviceConfiguration with memory_limit\n",
    "    # 2.  Just let TF grow the pool as needed:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(\"Memory growth on – no per-process hard cap.\")\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "#  Import model-building code\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "from demucs_v4_model import (\n",
    "    demucs_v4_fixed,  # the model-building function\n",
    "    ExpandDims,\n",
    "    ReduceMean,\n",
    "    LocalSelfAttention,\n",
    "    STFT,\n",
    "    InverseSTFT,\n",
    "    custom_loss,\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "#  Audio helpers\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "SR           = 44_100\n",
    "CHUNK_SECS   = 10\n",
    "CHUNK_SAMPLES = 441_000          # 441 000\n",
    "PADDED_LEN    = 441_000                  # 12-sample pad each side\n",
    "\n",
    "def load_mono(fp, sr=SR):\n",
    "    \"\"\"Return mono float32 audio in range [-1,1].\"\"\"\n",
    "    wav, _ = librosa.load(fp, sr=sr, mono=True)\n",
    "    return wav.astype(np.float32)\n",
    "\n",
    "def pad_or_trim(x, tgt_len=PADDED_LEN):\n",
    "    if len(x) < tgt_len:\n",
    "        return np.pad(x, (0, tgt_len - len(x)))\n",
    "    return x[:tgt_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Guitar': 'instrument_1', 'Drums': 'instrument_2', 'Piano': 'instrument_3', 'Bass': 'instrument_4', 'Strings (continued)': 'instrument_5', 'Organ': 'instrument_6', 'Synth Lead': 'instrument_7', 'Synth Pad': 'instrument_8', 'Chromatic Percussion': 'instrument_9', 'Brass': 'instrument_10', 'Pipe': 'instrument_11', 'Reed': 'instrument_12', 'Strings': 'instrument_13'}\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "#  Generator\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "INSTRUMENT_NAMES = [\n",
    "    \"Guitar\", \"Drums\", \"Piano\", \"Bass\", \"Strings (continued)\",\n",
    "    \"Organ\", \"Synth Lead\", \"Synth Pad\", \"Chromatic Percussion\",\n",
    "    \"Brass\", \"Pipe\", \"Reed\", \"Strings\"\n",
    "]\n",
    "MODEL_KEYS = {n: f\"instrument_{i+1}\" for i, n in enumerate(INSTRUMENT_NAMES)}\n",
    "print(MODEL_KEYS)\n",
    "\n",
    "def data_generator(root, batch_size=8):\n",
    "    \"\"\"Yields (mix, targets_dict) batches indefinitely.\"\"\"\n",
    "    root = os.path.expanduser(root)\n",
    "    track_dirs = [d for d in glob.glob(os.path.join(root, '*')) if os.path.isdir(d)]\n",
    "    n_tracks   = len(track_dirs)\n",
    "    chunk      = CHUNK_SAMPLES\n",
    "    #print(\"got here d1\")\n",
    "    \n",
    "\n",
    "    while True:            # epoch loop\n",
    "        random.shuffle(track_dirs)\n",
    "        #print(\"got here d2\")\n",
    "\n",
    "        for i in range(0, n_tracks, batch_size):\n",
    "            dirs = track_dirs[i:i + batch_size]\n",
    "\n",
    "            mixes   = []\n",
    "            targets = {k: [] for k in MODEL_KEYS.values()}\n",
    "\n",
    "            for d in dirs:\n",
    "                # ── grab full-length mix + stems ───────────────────\n",
    "                mix_files  = [f for f in os.listdir(d) if 'mix_chunk' in f.lower()]\n",
    "                #print(\"got here d3\")\n",
    "                if not mix_files:\n",
    "                    continue\n",
    "                mix_full = load_mono(os.path.join(d, mix_files[0]))\n",
    "\n",
    "                # random starting offset (if long enough)\n",
    "                if len(mix_full) > chunk:\n",
    "                    start = np.random.randint(0, len(mix_full) - chunk + 1)\n",
    "                    mix_clip = mix_full[start:start + chunk]\n",
    "                else:\n",
    "                    mix_clip = pad_or_trim(mix_full, chunk)\n",
    "\n",
    "                # get peak for joint scaling\n",
    "                peak = np.max(np.abs(mix_clip)) + 1e-7\n",
    "                mix_clip /= peak\n",
    "\n",
    "                # per-instrument\n",
    "                stem_dict = {}\n",
    "                for name in INSTRUMENT_NAMES:\n",
    "                    fmatch = next(\n",
    "                        (f for f in os.listdir(d)\n",
    "                         if name.lower() in f.lower() and '_chunk_' in f.lower()),\n",
    "                        None\n",
    "                    )\n",
    "                    if fmatch:\n",
    "                        full = load_mono(os.path.join(d, fmatch))\n",
    "                        if len(full) > chunk:\n",
    "                            stem = full[start:start + chunk]\n",
    "                        else:\n",
    "                            stem = pad_or_trim(full, chunk)\n",
    "                        stem = stem / peak                     # SAME scale\n",
    "                    else:\n",
    "                        stem = np.zeros(chunk, dtype=np.float32)\n",
    "                    stem_dict[name] = stem\n",
    "\n",
    "                # pad 12 samples each side\n",
    "                mix_pad = pad_or_trim(mix_clip, PADDED_LEN)\n",
    "                mixes.append(mix_pad)\n",
    "\n",
    "                for name in INSTRUMENT_NAMES:\n",
    "                    targets[MODEL_KEYS[name]].append(\n",
    "                        pad_or_trim(stem_dict[name], PADDED_LEN)[..., None]\n",
    "                    )\n",
    "\n",
    "            if not mixes:       # skip empty batch\n",
    "                continue\n",
    "\n",
    "            mix_batch = np.array(mixes, dtype=np.float32)[..., None]  # (B,441024,1)\n",
    "            tgt_batch = {k: np.array(v, dtype=np.float32)\n",
    "                         for k, v in targets.items()}\n",
    "            \n",
    "\n",
    "            yield mix_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9fH7SYGNKDbS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1761719416.448287   78469 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78761 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:61:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761719416.451082   78469 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78761 MB memory:  -> device: 1, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:62:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761719416.452652   78469 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78761 MB memory:  -> device: 2, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:63:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761719416.454238   78469 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 78761 MB memory:  -> device: 3, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:64:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761719416.455772   78469 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 78761 MB memory:  -> device: 4, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6a:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761719416.457316   78469 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 78761 MB memory:  -> device: 5, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6b:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761719416.458778   78469 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 78761 MB memory:  -> device: 6, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6c:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761719416.460297   78469 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 78761 MB memory:  -> device: 7, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6d:00.0, compute capability: 9.0\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_8', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_9', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_10', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_11', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_12', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_13', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_14', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_15', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#loaded model + loss function\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    # return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "\n",
    "#model = demucs_v4_fixed()\n",
    "#model.save(\"demucs_v4_fixed_model.keras\")   # no warnings\n",
    "reloaded = tf.keras.models.load_model(\n",
    "    \"demucs_v4_fixed_model.keras\",\n",
    "    custom_objects={\n",
    "        \"ExpandDims\":   ExpandDims,\n",
    "        \"ReduceMean\":   ReduceMean,\n",
    "        \"LocalSelfAttention\": LocalSelfAttention,\n",
    "        \"STFT\":         STFT,\n",
    "        \"InverseSTFT\":  InverseSTFT,\n",
    "        \"custom_loss\":  custom_loss,\n",
    "    },\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ScprZkvzb_p",
    "outputId": "640c8985-3159-42a2-cbc5-9d8566e6f3ef"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Model Evaluation\n",
    "\n",
    "#Parameters\n",
    "batch_size = 1\n",
    "steps_per_epoch = 3545\n",
    "\n",
    "# --- parameters ---\n",
    "NUM_INST   = 13\n",
    "BATCH_SIZE = 8\n",
    "TEST_STEPS = 3545                # = #batches you want to run\n",
    "\n",
    "test_dir = '~/madari3/gcs-bucket/Slakh_Dataset_Chunked/test_chunked'\n",
    "#test_gen = data_generator(test_data_dir, batch_size=batch_size)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Build the same Dataset wrapper used in training\n",
    "# ------------------------------------------------------------------\n",
    "# Compile only once; OK to reuse the same loss / optimizer\n",
    "reloaded.compile(optimizer='adam', loss=custom_loss, jit_compile=False, run_eagerly = False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made it\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753765925.681847    2716 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753765930.255780    2716 service.cc:152] XLA service 0x767c7c00bf40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753765930.255821    2716 service.cc:160]   StreamExecutor device (0): NVIDIA H100 PCIe, Compute Capability 9.0\n",
      "I0000 00:00:1753765931.013276    2716 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 465/3545\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41:27\u001b[0m 2s/step - instrument_10_loss: nan - instrument_11_loss: nan - instrument_12_loss: nan - instrument_13_loss: nan - instrument_1_loss: nan - instrument_2_loss: nan - instrument_3_loss: nan - instrument_4_loss: nan - instrument_5_loss: nan - instrument_6_loss: nan - instrument_7_loss: nan - instrument_8_loss: nan - instrument_9_loss: nan - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2558/3234434608.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProgbarLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m results = reloaded.evaluate(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTEST_STEPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_evaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"made it\")\n",
    "\n",
    "\n",
    "\n",
    "# Now evaluate\n",
    "test_ds = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(test_dir, BATCH_SIZE),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, PADDED_LEN, 1),  dtype=tf.float32),\n",
    "            {k: tf.TensorSpec(shape=(None, PADDED_LEN, 1), dtype=tf.float32)\n",
    "             for k in MODEL_KEYS.values()}\n",
    "        )\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "print(\"here\")\n",
    "# Optional: show progress even for slow batches\n",
    "from tensorflow.keras.callbacks import ProgbarLogger\n",
    "pb = ProgbarLogger()\n",
    "\n",
    "results = reloaded.evaluate(\n",
    "    test_ds,\n",
    "    steps=TEST_STEPS,\n",
    "    callbacks=[pb],     # progress bar\n",
    "    verbose=1           # prints the header immediately\n",
    ")\n",
    "\n",
    "#results = reloaded.evaluate(test_gen, steps=3545, return_dict=False, verbose = 1)\n",
    "\n",
    "print(\"Full results array:\", results)\n",
    "print(f\"Total loss: {results[0]:.4f}\")\n",
    "\n",
    "# Each subsequent entry in `results` corresponds to instrument_1, instrument_2, etc.\n",
    "for i in range(1, len(results)):\n",
    "    model_key = f\"instrument_{i}\"\n",
    "    # Convert \"instrument_X\" -> actual name, e.g. \"Guitar\"\n",
    "    for key in MODEL_KEYS:\n",
    "        if(MODEL_KEYS[key] == model_key):\n",
    "            instrument_name = key\n",
    "            print(f\"{instrument_name} loss: {results[i]:.4f}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezTE0rIc1ygJ",
    "outputId": "2bca50ec-4076-4941-ab6b-7cccc83bc8d9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1761720305.966347  102393 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78761 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:61:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761720305.968009  102393 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78761 MB memory:  -> device: 1, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:62:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761720305.969526  102393 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78761 MB memory:  -> device: 2, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:63:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761720305.971003  102393 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 78761 MB memory:  -> device: 3, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:64:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761720305.972473  102393 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 78761 MB memory:  -> device: 4, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6a:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761720305.974493  102393 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 78761 MB memory:  -> device: 5, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6b:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761720305.975964  102393 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 78761 MB memory:  -> device: 6, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6c:00.0, compute capability: 9.0\n",
      "I0000 00:00:1761720305.977399  102393 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 78761 MB memory:  -> device: 7, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6d:00.0, compute capability: 9.0\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_8', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_9', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_10', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_11', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_12', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_13', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_14', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'local_self_attention_15', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761720310.300208  103226 service.cc:152] XLA service 0x76d8d8002030 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1761720310.300240  103226 service.cc:160]   StreamExecutor device (0): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1761720310.300243  103226 service.cc:160]   StreamExecutor device (1): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1761720310.300245  103226 service.cc:160]   StreamExecutor device (2): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1761720310.300247  103226 service.cc:160]   StreamExecutor device (3): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1761720310.300248  103226 service.cc:160]   StreamExecutor device (4): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1761720310.300249  103226 service.cc:160]   StreamExecutor device (5): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1761720310.300262  103226 service.cc:160]   StreamExecutor device (6): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1761720310.300265  103226 service.cc:160]   StreamExecutor device (7): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "2025-10-29 06:45:10.430885: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1761720311.160481  103226 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "2025-10-29 06:45:15.316124: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_25', 296 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:16.492077: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_31', 296 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:16.539525: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 296 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:16.572362: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_62_0', 296 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:16.832869: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 528 bytes spill stores, 528 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:17.349223: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_70', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:17.549742: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_64', 340 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:18.820428: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_64_0', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:19.797758: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 580 bytes spill stores, 596 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:20.037845: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_66', 1112 bytes spill stores, 888 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:20.355981: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_68', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:20.455758: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_70', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:20.937428: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_64', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:21.218392: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_68', 228 bytes spill stores, 228 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:21.638772: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_64', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:21.730748: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_63_0', 296 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-10-29 06:45:21.847656: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_32', 296 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "I0000 00:00:1761720330.978621  103226 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Separate a full-length mix with a Demucs-V4-Fixed model.\n",
    "→ One WAV file per instrument in <output_dir>.\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, math, random, sys\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import tensorflow as tf\n",
    "from pathlib import Path  \n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Constants from training\n",
    "# ------------------------------------------------------------------\n",
    "TARGET_LEN = 441_000\n",
    "SR           = 44_100\n",
    "\n",
    "INSTRUMENT_NAMES = [\n",
    "    \"Guitar\", \"Drums\", \"Piano\", \"Bass\", \"Strings (continued)\",\n",
    "    \"Organ\", \"Synth Lead\", \"Synth Pad\", \"Chromatic Percussion\",\n",
    "    \"Brass\", \"Pipe\", \"Reed\", \"Strings\"\n",
    "]\n",
    "MODEL_KEYS = {f\"instrument_{i+1}\": name for i, name in enumerate(INSTRUMENT_NAMES)}\n",
    "# e.g.  'instrument_1' → 'Guitar'\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Utility helpers\n",
    "# ------------------------------------------------------------------\n",
    "def pad_or_trim(x: np.ndarray, length: int) -> np.ndarray:\n",
    "    \"\"\"Zero-pad (left+right) or trim to exactly `length` samples.\"\"\"\n",
    "    if len(x) >= length:\n",
    "        return x[:length]\n",
    "    pad = length - len(x)\n",
    "    left = pad // 2\n",
    "    right = pad - left\n",
    "    return np.pad(x, (left, right))\n",
    "\n",
    "def chunk_audio(wave: np.ndarray, chunk_size: int):\n",
    "    \"\"\"\n",
    "    Yield non-overlapping chunks of exactly `chunk_size` samples.\n",
    "    The last chunk is zero-padded **at the end only** so time alignment\n",
    "    is preserved.\n",
    "    \"\"\"\n",
    "    for start in range(0, len(wave), chunk_size):\n",
    "        end   = min(start + chunk_size, len(wave))\n",
    "        chunk = wave[start:end]\n",
    "\n",
    "        if len(chunk) < chunk_size:          # last chunk\n",
    "            pad = chunk_size - len(chunk)\n",
    "            chunk = np.pad(chunk, (0, pad))  # pad **after** the audio\n",
    "\n",
    "        yield chunk\n",
    "\n",
    "def model_chunk_len(model):\n",
    "    # Keras stores the time dimension at index 1 for (batch, time, channels)\n",
    "    tdim = model.input_shape[1]\n",
    "    if tdim is None:\n",
    "        # Fallback: try to find a custom layer with .target_len, or default\n",
    "        tdim = next((getattr(l, \"target_len\", None) for l in model.layers\n",
    "                    if hasattr(l, \"target_len\") and getattr(l, \"target_len\")), 441024)\n",
    "    return int(tdim)\n",
    "\n",
    "\n",
    "\n",
    "def load_audio(path, sr=44100):\n",
    "    p = Path(path).expanduser().resolve()\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Audio file not found: {p}\")\n",
    "    wav, file_sr = sf.read(str(p), always_2d=False)  # pass str(), not Path\n",
    "    if file_sr != sr:\n",
    "        import librosa\n",
    "        wav = librosa.resample(wav, orig_sr=file_sr, target_sr=sr)\n",
    "    if wav.ndim == 2:\n",
    "        wav = wav.mean(axis=1)\n",
    "    return wav.astype(np.float32)\n",
    "\n",
    "\n",
    "def separate_long_audio(model, audio_path, output_dir, sr=44100):\n",
    "    #----paths-----\n",
    "    audio_path  = Path(audio_path).expanduser().resolve()\n",
    "    output_dir  = Path(output_dir).expanduser().resolve()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    wav, file_sr = sf.read(str(audio_path), always_2d=False)\n",
    "    if file_sr != sr:\n",
    "        import librosa\n",
    "        wav = librosa.resample(wav, orig_sr=file_sr, target_sr=sr)\n",
    "    if wav.ndim == 2:\n",
    "        wav = wav.mean(axis=1)\n",
    "    wav = wav.astype(np.float32)\n",
    "    global_peak = np.max(np.abs(wav)) + 1e-7\n",
    "\n",
    "    # --- sizes from model ---\n",
    "    CHUNK = model_chunk_len(model)          # <- 441024 for your model\n",
    "    HOP   = TARGET_LEN // 2                   # 50% overlap on predicted length\n",
    "    WIN   = np.sqrt(np.hanning(TARGET_LEN).astype(np.float32))\n",
    "    OFFSET = (CHUNK - TARGET_LEN) // 2\n",
    "\n",
    "    # prepare OLA buffers per output head\n",
    "    keys = list(MODEL_KEYS.keys())              # 'instrument_1', ...\n",
    "    N    = len(wav)\n",
    "    acc  = {k: np.zeros(N + CHUNK, np.float32) for k in keys}\n",
    "    wsum = np.zeros(N + CHUNK, np.float32)\n",
    "\n",
    "    def predict_chunk(x):\n",
    "        x = x.astype(np.float32)\n",
    "        peak = global_peak\n",
    "        x_norm = x / peak  # global peak-normalize to match training scale\n",
    "        if len(x_norm) < CHUNK:\n",
    "            x_norm = np.pad(x_norm, (0, CHUNK - len(x_norm)))\n",
    "        outs = model.predict(x_norm[np.newaxis, :, np.newaxis], verbose=0)\n",
    "        # Normalize return type to dict keyed by instrument names\n",
    "        if isinstance(outs, list):\n",
    "            pred_dict = {k: v for k, v in zip(keys, outs)}\n",
    "        elif isinstance(outs, dict):\n",
    "            if keys and keys[0] in outs:\n",
    "                pred_dict = outs\n",
    "            else:\n",
    "                out_names = list(getattr(model, \"output_names\", []))\n",
    "                pred_dict = {k: outs[n] for k, n in zip(keys, out_names)} if out_names else {k: v for k, v in zip(keys, outs.values())}\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported prediction return type: expected list or dict.\")\n",
    "        return {k: pred_dict[k][0, :TARGET_LEN, 0] * peak for k in keys}\n",
    "\n",
    "\n",
    "\n",
    "    # stream over overlapping windows\n",
    "    for start in range(0, N, HOP):\n",
    "        end   = min(start + CHUNK, N)\n",
    "        chunk = wav[start:end]\n",
    "        pred  = predict_chunk(chunk)\n",
    "        w = WIN\n",
    "        if end - start < CHUNK:             # shorten window at tail if we didn’t pad\n",
    "            # we still padded for the net, so use full window; crop when adding\n",
    "            pass\n",
    "\n",
    "        for k in keys:\n",
    "            acc[k][start+OFFSET:start+OFFSET+TARGET_LEN] += pred[k] * WIN\n",
    "            wsum[start+OFFSET:start+OFFSET+TARGET_LEN] += WIN**2\n",
    "\n",
    "    # normalize by overlap weights and trim to original length\n",
    "    eps = 1e-8\n",
    "    out = {k: (acc[k][:N] / np.maximum(wsum[:N], eps)) for k in keys}\n",
    "\n",
    "    # save with human names\n",
    "    names = [\n",
    "        \"Guitar\",\"Drums\",\"Piano\",\"Bass\",\"Strings (continued)\",\n",
    "        \"Organ\",\"Synth Lead\",\"Synth Pad\",\"Chromatic Percussion\",\n",
    "        \"Brass\",\"Pipe\",\"Reed\",\"Strings\"\n",
    "    ]\n",
    "    key_to_name = {f\"instrument_{i+1}\": n for i, n in enumerate(names)}\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    for k, y in out.items():\n",
    "        sf.write(Path(output_dir, f\"{key_to_name.get(k,k)}.wav\"), y.astype(np.float32), sr, subtype=\"FLOAT\")\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  Entrypoint\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = tf.keras.models.load_model(\n",
    "        \"demucs_v4_fixed_model.keras\",\n",
    "        custom_objects={\n",
    "            \"STFT\": STFT,\n",
    "            \"InverseSTFT\": InverseSTFT,\n",
    "            \"LocalSelfAttention\": LocalSelfAttention,\n",
    "        },\n",
    "        compile=False,\n",
    "    )\n",
    "\n",
    "    test_mix   = \"~/madari3/1508 mix.wav\"\n",
    "    output_dir = \"~/madari3/output\"\n",
    "\n",
    "    separate_long_audio(model, test_mix, output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Cloud IDE)",
   "language": "python",
   "name": "cloud-ide"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
