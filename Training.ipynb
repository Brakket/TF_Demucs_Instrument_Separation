{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b564b6fa",
   "metadata": {},
   "source": [
    "# Hybrid Demucs V4 Training Pipeline\n",
    "\n",
    "This notebook trains a TensorFlow implementation of the Hybrid Demucs model for music source separation. The model separates mixed audio into 13 individual instrument stems.\n",
    "\n",
    "## Features\n",
    "- **Multi-GPU training** using `MirroredStrategy`\n",
    "- **Mixed precision** (BFloat16) for faster training on H100 GPUs\n",
    "- **Checkpoint resumption** - automatically resumes from best checkpoint\n",
    "- **Data augmentation** via random chunk selection from longer audio files\n",
    "\n",
    "## Dataset\n",
    "Uses the Slakh2100 dataset, pre-chunked into training examples with separated instrument stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad7475-42dd-4b46-a4e4-824815eafea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 05:15:03.290362: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-01 05:15:03.418521: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759295703.471979   40866 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759295703.487935   40866 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1759295703.589802   40866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759295703.589822   40866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759295703.589825   40866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759295703.589826   40866 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-01 05:15:03.604695: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Imports and Configuration\n",
    "# ==============================================================================\n",
    "\n",
    "import os, random, glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf  # TensorFlow 2.19+\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "from demucs_v4_model import demucs_v4_fixed, custom_loss\n",
    "\n",
    "# ==============================================================================\n",
    "# Audio Constants\n",
    "# ==============================================================================\n",
    "\n",
    "SR            = 44_100      # Sample rate (44.1 kHz)\n",
    "CHUNK_SECS    = 10          # Duration of each training chunk\n",
    "CHUNK_SAMPLES = 441_000     # 10 seconds * 44100 samples/sec\n",
    "PADDED_LEN    = 441_000     # Model input length\n",
    "\n",
    "# ==============================================================================\n",
    "# Audio Utility Functions\n",
    "# ==============================================================================\n",
    "\n",
    "def load_mono(fp, sr=SR):\n",
    "    \"\"\"Load audio file as mono float32 in range [-1, 1].\"\"\"\n",
    "    wav, _ = librosa.load(fp, sr=sr, mono=True)\n",
    "    wav = wav.astype(np.float32)\n",
    "    # Sanitize: replace NaN/Inf and clip to valid range\n",
    "    wav = np.nan_to_num(wav, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    wav = np.clip(wav, -1.0, 1.0)\n",
    "    return wav\n",
    "\n",
    "def pad_or_trim(x, tgt_len=PADDED_LEN):\n",
    "    \"\"\"Pad with zeros or trim audio to exact target length.\"\"\"\n",
    "    if len(x) < tgt_len:\n",
    "        return np.pad(x, (0, tgt_len - len(x)))\n",
    "    return x[:tgt_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e90ae",
   "metadata": {},
   "source": [
    "## Data Generator\n",
    "\n",
    "The generator yields batches of (mix, stems_dict) pairs indefinitely for training. Each batch contains:\n",
    "- **mix**: The combined audio of all instruments `(batch, 441000, 1)`\n",
    "- **stems_dict**: Dictionary mapping instrument names to separated audio `{instrument_i: (batch, 441000, 1)}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3287395-0dfb-4b7a-9edc-69893bc18406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target instrument stems (13 total)\n",
    "INSTRUMENT_NAMES = [\n",
    "    \"Guitar\", \"Drums\", \"Piano\", \"Bass\", \"Strings (continued)\",\n",
    "    \"Organ\", \"Synth Lead\", \"Synth Pad\", \"Chromatic Percussion\",\n",
    "    \"Brass\", \"Pipe\", \"Reed\", \"Strings\"\n",
    "]\n",
    "# Map instrument names to model output keys\n",
    "MODEL_KEYS = {n: f\"instrument_{i+1}\" for i, n in enumerate(INSTRUMENT_NAMES)}\n",
    "\n",
    "\n",
    "def data_generator(root, batch_size=8):\n",
    "    \"\"\"\n",
    "    Infinite generator yielding (mix, targets_dict) batches for training.\n",
    "    \n",
    "    Args:\n",
    "        root: Path to directory containing track subdirectories\n",
    "        batch_size: Number of samples per batch\n",
    "    \n",
    "    Yields:\n",
    "        Tuple of (mix_batch, targets_dict) where:\n",
    "        - mix_batch: (batch, PADDED_LEN, 1) mixed audio\n",
    "        - targets_dict: {instrument_key: (batch, PADDED_LEN, 1)} stem audio\n",
    "    \"\"\"\n",
    "    root = os.path.expanduser(root)\n",
    "    track_dirs = [d for d in glob.glob(os.path.join(root, '*')) if os.path.isdir(d)]\n",
    "    n_tracks   = len(track_dirs)\n",
    "    chunk      = CHUNK_SAMPLES\n",
    "\n",
    "    while True:            # epoch loop\n",
    "        random.shuffle(track_dirs)\n",
    "\n",
    "        for i in range(0, n_tracks, batch_size):\n",
    "            dirs = track_dirs[i:i + batch_size]\n",
    "            if len(dirs) < batch_size:\n",
    "                continue\n",
    "\n",
    "            mixes   = []\n",
    "            targets = {k: [] for k in MODEL_KEYS.values()}\n",
    "\n",
    "            for d in dirs:\n",
    "                # Load the mix audio file\n",
    "                mix_files  = [f for f in os.listdir(d) if 'mix_chunk' in f.lower()]\n",
    "                if not mix_files:\n",
    "                    continue\n",
    "                mix_full = load_mono(os.path.join(d, mix_files[0]))\n",
    "\n",
    "                # random starting offset (if long enough)\n",
    "                if len(mix_full) > chunk:\n",
    "                    start = np.random.randint(0, len(mix_full) - chunk + 1)\n",
    "                    mix_clip = mix_full[start:start + chunk]\n",
    "                else:\n",
    "                    mix_clip = pad_or_trim(mix_full, chunk)\n",
    "\n",
    "                # Peak-normalize for training stability\n",
    "                peak = float(np.nanmax(np.abs(mix_clip)))\n",
    "                if not np.isfinite(peak) or peak < 1e-4:\n",
    "                    peak = 1.0  # Treat near-silence as silence\n",
    "                mix_clip = mix_clip / peak\n",
    "\n",
    "                # Load and normalize each instrument stem\n",
    "                stem_dict = {}\n",
    "                for name in INSTRUMENT_NAMES:\n",
    "                    fmatch = next(\n",
    "                        (f for f in os.listdir(d)\n",
    "                         if f.lower().startswith(name.lower()+'_chunk_')),\n",
    "                        None\n",
    "                    )\n",
    "                    if fmatch:\n",
    "                        full = load_mono(os.path.join(d, fmatch))\n",
    "                        full = np.nan_to_num(full, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                        if len(full) > chunk:\n",
    "                            stem = full[start:start + chunk]\n",
    "                        else:\n",
    "                            stem = pad_or_trim(full, chunk)\n",
    "                        stem = stem / peak  # Use same normalization as mix\n",
    "                    else:\n",
    "                        stem = np.zeros(chunk, dtype=np.float32)\n",
    "                    stem_dict[name] = stem\n",
    "\n",
    "                # Ensure exact length for model input\n",
    "                mix_pad = pad_or_trim(mix_clip, PADDED_LEN)\n",
    "                mixes.append(mix_pad)\n",
    "\n",
    "                for name in INSTRUMENT_NAMES:\n",
    "                    targets[MODEL_KEYS[name]].append(\n",
    "                        pad_or_trim(stem_dict[name], PADDED_LEN)[..., None]\n",
    "                    )\n",
    "\n",
    "            if not mixes:\n",
    "                continue  # Skip empty batch\n",
    "\n",
    "            # Convert to numpy arrays with channel dimension\n",
    "            mix_batch = np.array(mixes, dtype=np.float32)[..., None]\n",
    "            tgt_batch = {k: np.array(v, dtype=np.float32) for k, v in targets.items()}\n",
    "\n",
    "            yield mix_batch, tgt_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7dc71",
   "metadata": {},
   "source": [
    "## Checkpoint Management\n",
    "\n",
    "Utilities for finding the best checkpoint and exporting the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96388e1b-450f-457f-bc39-26d2983b5184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, tensorflow as tf\n",
    "\n",
    "# Configuration - adjust these paths for your project\n",
    "checkpoint_dir = \"demucs_v4_fixed_ckpt\"       # Directory for weight checkpoints\n",
    "output_model   = \"demucs_v4_fixed_model.keras\"  # Final exported model path\n",
    "INPUT_SHAPE    = (PADDED_LEN, 1)              # Must match training input shape\n",
    "\n",
    "def _best_weights_path(ckpt_dir: str) -> str:\n",
    "    \"\"\"Find the best checkpoint file by parsing loss values from filenames.\"\"\"\n",
    "    paths = glob.glob(os.path.join(ckpt_dir, \"*.weights.h5\"))\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No .weights.h5 files in {ckpt_dir}\")\n",
    "\n",
    "    # Parse loss value from checkpoint filenames (e.g., \"ckpt_e01_loss0.123456.weights.h5\")\n",
    "    loss_re = re.compile(r\"_(?:val)?[lL]oss([0-9.]+)\\.weights\\.h5$\")\n",
    "\n",
    "    scored = []\n",
    "    for p in paths:\n",
    "        m = loss_re.search(p)\n",
    "        if m:\n",
    "            try:\n",
    "                scored.append((float(m.group(1)), p))\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    if scored:\n",
    "        scored.sort(key=lambda t: t[0])  # Lowest loss = best\n",
    "        best_loss, best_path = scored[0]\n",
    "        print(f\"Selected best by loss: {os.path.basename(best_path)} (loss={best_loss:.6f})\")\n",
    "        return best_path\n",
    "\n",
    "    # Fallback: use newest file by modification time\n",
    "    best_path = max(paths, key=os.path.getmtime)\n",
    "    print(f\"Selected newest weights: {os.path.basename(best_path)}\")\n",
    "    return best_path\n",
    "\n",
    "\n",
    "def export_model_from_best():\n",
    "    \"\"\"Load best weights and export complete model with architecture.\"\"\"\n",
    "    best_path = _best_weights_path(checkpoint_dir)\n",
    "\n",
    "    # Rebuild model graph (must match training architecture)\n",
    "    model = demucs_v4_fixed(INPUT_SHAPE)\n",
    "    model.load_weights(best_path)\n",
    "    print(f\"Loaded weights from {best_path}\")\n",
    "\n",
    "    # Save full model (architecture + weights)\n",
    "    model.save(output_model)\n",
    "    print(f\"Saved full model to {output_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a52d27",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "The main training loop with:\n",
    "- Multi-GPU distribution strategy\n",
    "- Loss-scaled optimizer for mixed precision stability\n",
    "- Best and last checkpoint callbacks\n",
    "- Early stopping on NaN loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410d9d3-b7d5-4064-9260-93a587886777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_demucs_v4_fixed(data_dir,\n",
    "                          checkpoint_dir='demucs_v4_fixed_ckpt',\n",
    "                          batch_size=2,\n",
    "                          steps_per_epoch=1275,\n",
    "                          epochs=25):\n",
    "    \"\"\"\n",
    "    Train the Hybrid Demucs V4 model with multi-GPU support.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Path to training data directory\n",
    "        checkpoint_dir: Directory to save weight checkpoints\n",
    "        batch_size: Samples per batch (per replica)\n",
    "        steps_per_epoch: Training steps before epoch ends\n",
    "        epochs: Total training epochs\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Multi-GPU training with MirroredStrategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        # Optimizer with gradient clipping and loss scaling for mixed precision\n",
    "        base_opt = tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=0.5)\n",
    "        opt = mixed_precision.LossScaleOptimizer(base_opt, dynamic=True)\n",
    "\n",
    "        # Build and compile model\n",
    "        model = demucs_v4_fixed((PADDED_LEN, 1))\n",
    "        model.compile(\n",
    "            optimizer=opt,\n",
    "            loss=custom_loss,\n",
    "            jit_compile=False,\n",
    "            run_eagerly=False\n",
    "        )\n",
    "\n",
    "        # Resume from checkpoint if available\n",
    "        try:\n",
    "            best = _best_weights_path(checkpoint_dir)\n",
    "            print(f\"Resuming from: {os.path.basename(best)}\")\n",
    "            model.load_weights(best)\n",
    "        except Exception:\n",
    "            print(\"Starting training from scratch\")\n",
    "\n",
    "    # Callback: Save best checkpoint by loss\n",
    "    ckpt_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, \"ckpt_e{epoch:02d}_loss{loss:.6f}.weights.h5\"),\n",
    "        monitor=\"loss\",\n",
    "        mode=\"min\",\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        save_freq='epoch',\n",
    "    )\n",
    "\n",
    "    # Callback: Always save latest checkpoint\n",
    "    ckpt_last = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, 'last.weights.h5'),\n",
    "        save_weights_only=True,\n",
    "        save_best_only=False,\n",
    "        save_freq='epoch'\n",
    "    )\n",
    "\n",
    "    # Callback: Early stopping and NaN detection\n",
    "    es_cb = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "    nan_cb = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "    # Create tf.data.Dataset with prefetching\n",
    "    option = tf.data.Options()\n",
    "    option.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(data_dir, batch_size),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, PADDED_LEN, 1),  dtype=tf.float32),\n",
    "            {k: tf.TensorSpec(shape=(None, PADDED_LEN, 1), dtype=tf.float32)\n",
    "             for k in MODEL_KEYS.values()}\n",
    "        )\n",
    "    ).prefetch(tf.data.AUTOTUNE).with_options(option)\n",
    "\n",
    "    # Train the model\n",
    "    try:\n",
    "        model.fit(\n",
    "            ds,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            callbacks=[ckpt_best, ckpt_last, es_cb, nan_cb],\n",
    "            verbose=1,\n",
    "        )\n",
    "    finally:\n",
    "        # Export best model when training completes (or fails)\n",
    "        try:\n",
    "            export_model_from_best()\n",
    "        except FileNotFoundError:\n",
    "            print(\"No checkpoints found; exporting current model instead.\")\n",
    "            model.save(output_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46525739",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Execute training on the Slakh dataset. Adjust `batch_size`, `steps_per_epoch`, and `epochs` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e383014f-ec41-4a6a-b7ef-769521456a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1759295710.532050   40866 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78761 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:61:00.0, compute capability: 9.0\n",
      "I0000 00:00:1759295710.533660   40866 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78761 MB memory:  -> device: 1, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:62:00.0, compute capability: 9.0\n",
      "I0000 00:00:1759295710.535184   40866 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78761 MB memory:  -> device: 2, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:63:00.0, compute capability: 9.0\n",
      "I0000 00:00:1759295710.536683   40866 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 78761 MB memory:  -> device: 3, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:64:00.0, compute capability: 9.0\n",
      "I0000 00:00:1759295710.538164   40866 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 78761 MB memory:  -> device: 4, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6a:00.0, compute capability: 9.0\n",
      "I0000 00:00:1759295710.540129   40866 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 78761 MB memory:  -> device: 5, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6b:00.0, compute capability: 9.0\n",
      "I0000 00:00:1759295710.541572   40866 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 78761 MB memory:  -> device: 6, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6c:00.0, compute capability: 9.0\n",
      "I0000 00:00:1759295710.543018   40866 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 78761 MB memory:  -> device: 7, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:6d:00.0, compute capability: 9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Selected best by loss: ckpt_e01_loss2.263522.weights.h5 (loss=2.263522)\n",
      "Resuming from best: ckpt_e01_loss2.263522.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 582 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/usr/lib/python3/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 578 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 05:15:58.743859: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: CANCELLED: GetNextFromShard was cancelled\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "2025-10-01 05:15:58.743974: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: CANCELLED: GetNextFromShard was cancelled\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]] [type.googleapis.com/tensorflow.DerivedStatus='']\n",
      "2025-10-01 05:15:58.745186: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: CANCELLED: GetNextFromShard was cancelled\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]] [type.googleapis.com/tensorflow.DerivedStatus='']\n",
      "2025-10-01 05:15:58.747352: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: CANCELLED: GetNextFromShard was cancelled\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]] [type.googleapis.com/tensorflow.DerivedStatus='']\n",
      "2025-10-01 05:15:58.751377: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: CANCELLED: GetNextFromShard was cancelled\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]] [type.googleapis.com/tensorflow.DerivedStatus='']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 288 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1759296013.812500   41871 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "I0000 00:00:1759296013.838119   41826 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "I0000 00:00:1759296013.843943   41789 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "I0000 00:00:1759296013.847592   41879 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "I0000 00:00:1759296013.851580   41857 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "I0000 00:00:1759296013.858075   41866 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "I0000 00:00:1759296013.860534   41847 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "I0000 00:00:1759296013.862422   41831 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759296021.090434   41805 service.cc:152] XLA service 0x7b7e68811e30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1759296021.090461   41805 service.cc:160]   StreamExecutor device (0): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1759296021.090466   41805 service.cc:160]   StreamExecutor device (1): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1759296021.090468   41805 service.cc:160]   StreamExecutor device (2): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1759296021.090469   41805 service.cc:160]   StreamExecutor device (3): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1759296021.090470   41805 service.cc:160]   StreamExecutor device (4): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1759296021.090472   41805 service.cc:160]   StreamExecutor device (5): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1759296021.090473   41805 service.cc:160]   StreamExecutor device (6): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1759296021.090474   41805 service.cc:160]   StreamExecutor device (7): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1759296021.876211   41867 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6726s\u001b[0m 5s/step - instrument_10_loss: 0.0923 - instrument_11_loss: 0.0651 - instrument_12_loss: 0.1392 - instrument_13_loss: 0.0725 - instrument_1_loss: 0.2797 - instrument_2_loss: 0.1242 - instrument_3_loss: 0.2658 - instrument_4_loss: 0.3364 - instrument_5_loss: 0.2206 - instrument_6_loss: 0.1244 - instrument_7_loss: 0.0938 - instrument_8_loss: 0.1555 - instrument_9_loss: 0.0765 - loss: 2.0459\n",
      "Epoch 2/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5415s\u001b[0m 4s/step - instrument_10_loss: 0.0621 - instrument_11_loss: 0.0456 - instrument_12_loss: 0.0948 - instrument_13_loss: 0.0564 - instrument_1_loss: 0.2541 - instrument_2_loss: 0.0957 - instrument_3_loss: 0.2406 - instrument_4_loss: 0.2952 - instrument_5_loss: 0.1967 - instrument_6_loss: 0.1055 - instrument_7_loss: 0.0694 - instrument_8_loss: 0.1302 - instrument_9_loss: 0.0446 - loss: 1.6910\n",
      "Epoch 3/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5462s\u001b[0m 4s/step - instrument_10_loss: 0.0581 - instrument_11_loss: 0.0414 - instrument_12_loss: 0.0877 - instrument_13_loss: 0.0499 - instrument_1_loss: 0.2501 - instrument_2_loss: 0.0874 - instrument_3_loss: 0.2340 - instrument_4_loss: 0.2820 - instrument_5_loss: 0.1904 - instrument_6_loss: 0.1002 - instrument_7_loss: 0.0618 - instrument_8_loss: 0.1254 - instrument_9_loss: 0.0379 - loss: 1.6062\n",
      "Epoch 4/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5419s\u001b[0m 4s/step - instrument_10_loss: 0.0491 - instrument_11_loss: 0.0368 - instrument_12_loss: 0.0699 - instrument_13_loss: 0.0449 - instrument_1_loss: 0.2398 - instrument_2_loss: 0.0793 - instrument_3_loss: 0.2170 - instrument_4_loss: 0.2658 - instrument_5_loss: 0.1806 - instrument_6_loss: 0.0891 - instrument_7_loss: 0.0522 - instrument_8_loss: 0.1165 - instrument_9_loss: 0.0291 - loss: 1.4700\n",
      "Epoch 5/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5556s\u001b[0m 4s/step - instrument_10_loss: 0.0253 - instrument_11_loss: 0.0277 - instrument_12_loss: 0.0415 - instrument_13_loss: 0.0335 - instrument_1_loss: 0.2274 - instrument_2_loss: 0.0597 - instrument_3_loss: 0.2127 - instrument_4_loss: 0.2554 - instrument_5_loss: 0.1593 - instrument_6_loss: 0.0753 - instrument_7_loss: 0.0353 - instrument_8_loss: 0.0979 - instrument_9_loss: 0.0164 - loss: 1.2673\n",
      "Epoch 6/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5560s\u001b[0m 4s/step - instrument_10_loss: 0.0286 - instrument_11_loss: 0.0294 - instrument_12_loss: 0.0390 - instrument_13_loss: 0.0367 - instrument_1_loss: 0.2339 - instrument_2_loss: 0.0565 - instrument_3_loss: 0.2050 - instrument_4_loss: 0.2468 - instrument_5_loss: 0.1693 - instrument_6_loss: 0.0804 - instrument_7_loss: 0.0381 - instrument_8_loss: 0.1062 - instrument_9_loss: 0.0163 - loss: 1.2862\n",
      "Epoch 7/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5730s\u001b[0m 4s/step - instrument_10_loss: 0.0243 - instrument_11_loss: 0.0312 - instrument_12_loss: 0.0379 - instrument_13_loss: 0.0367 - instrument_1_loss: 0.2291 - instrument_2_loss: 0.0523 - instrument_3_loss: 0.1996 - instrument_4_loss: 0.2312 - instrument_5_loss: 0.1633 - instrument_6_loss: 0.0785 - instrument_7_loss: 0.0376 - instrument_8_loss: 0.1019 - instrument_9_loss: 0.0153 - loss: 1.2388\n",
      "Epoch 8/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5687s\u001b[0m 4s/step - instrument_10_loss: 0.0261 - instrument_11_loss: 0.0300 - instrument_12_loss: 0.0370 - instrument_13_loss: 0.0323 - instrument_1_loss: 0.2317 - instrument_2_loss: 0.0502 - instrument_3_loss: 0.1983 - instrument_4_loss: 0.2268 - instrument_5_loss: 0.1717 - instrument_6_loss: 0.0773 - instrument_7_loss: 0.0377 - instrument_8_loss: 0.0991 - instrument_9_loss: 0.0143 - loss: 1.2326\n",
      "Epoch 9/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5684s\u001b[0m 4s/step - instrument_10_loss: 0.0287 - instrument_11_loss: 0.0285 - instrument_12_loss: 0.0387 - instrument_13_loss: 0.0336 - instrument_1_loss: 0.2311 - instrument_2_loss: 0.0496 - instrument_3_loss: 0.2075 - instrument_4_loss: 0.2262 - instrument_5_loss: 0.1634 - instrument_6_loss: 0.0785 - instrument_7_loss: 0.0367 - instrument_8_loss: 0.1058 - instrument_9_loss: 0.0152 - loss: 1.2434\n",
      "Epoch 10/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5602s\u001b[0m 4s/step - instrument_10_loss: 0.0267 - instrument_11_loss: 0.0288 - instrument_12_loss: 0.0374 - instrument_13_loss: 0.0378 - instrument_1_loss: 0.2245 - instrument_2_loss: 0.0475 - instrument_3_loss: 0.2018 - instrument_4_loss: 0.2221 - instrument_5_loss: 0.1597 - instrument_6_loss: 0.0815 - instrument_7_loss: 0.0370 - instrument_8_loss: 0.1031 - instrument_9_loss: 0.0145 - loss: 1.2224\n",
      "Epoch 11/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5786s\u001b[0m 5s/step - instrument_10_loss: 0.0232 - instrument_11_loss: 0.0301 - instrument_12_loss: 0.0392 - instrument_13_loss: 0.0359 - instrument_1_loss: 0.2264 - instrument_2_loss: 0.0452 - instrument_3_loss: 0.2054 - instrument_4_loss: 0.2107 - instrument_5_loss: 0.1632 - instrument_6_loss: 0.0762 - instrument_7_loss: 0.0345 - instrument_8_loss: 0.1041 - instrument_9_loss: 0.0155 - loss: 1.2096\n",
      "Epoch 12/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6960s\u001b[0m 5s/step - instrument_10_loss: 0.0263 - instrument_11_loss: 0.0283 - instrument_12_loss: 0.0372 - instrument_13_loss: 0.0356 - instrument_1_loss: 0.2214 - instrument_2_loss: 0.0475 - instrument_3_loss: 0.1990 - instrument_4_loss: 0.2105 - instrument_5_loss: 0.1629 - instrument_6_loss: 0.0777 - instrument_7_loss: 0.0391 - instrument_8_loss: 0.1038 - instrument_9_loss: 0.0148 - loss: 1.2041\n",
      "Epoch 13/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5462s\u001b[0m 4s/step - instrument_10_loss: 0.0261 - instrument_11_loss: 0.0289 - instrument_12_loss: 0.0365 - instrument_13_loss: 0.0352 - instrument_1_loss: 0.2256 - instrument_2_loss: 0.0437 - instrument_3_loss: 0.2013 - instrument_4_loss: 0.2125 - instrument_5_loss: 0.1631 - instrument_6_loss: 0.0771 - instrument_7_loss: 0.0363 - instrument_8_loss: 0.1038 - instrument_9_loss: 0.0149 - loss: 1.2049\n",
      "Epoch 14/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5410s\u001b[0m 4s/step - instrument_10_loss: 0.0254 - instrument_11_loss: 0.0282 - instrument_12_loss: 0.0410 - instrument_13_loss: 0.0347 - instrument_1_loss: 0.2255 - instrument_2_loss: 0.0439 - instrument_3_loss: 0.2003 - instrument_4_loss: 0.2113 - instrument_5_loss: 0.1605 - instrument_6_loss: 0.0785 - instrument_7_loss: 0.0367 - instrument_8_loss: 0.1032 - instrument_9_loss: 0.0161 - loss: 1.2055\n",
      "Epoch 15/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6256s\u001b[0m 5s/step - instrument_10_loss: 0.0258 - instrument_11_loss: 0.0299 - instrument_12_loss: 0.0381 - instrument_13_loss: 0.0375 - instrument_1_loss: 0.2245 - instrument_2_loss: 0.0431 - instrument_3_loss: 0.2019 - instrument_4_loss: 0.2161 - instrument_5_loss: 0.1608 - instrument_6_loss: 0.0779 - instrument_7_loss: 0.0376 - instrument_8_loss: 0.1003 - instrument_9_loss: 0.0150 - loss: 1.2087\n",
      "Epoch 16/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6205s\u001b[0m 5s/step - instrument_10_loss: 0.0251 - instrument_11_loss: 0.0281 - instrument_12_loss: 0.0409 - instrument_13_loss: 0.0343 - instrument_1_loss: 0.2286 - instrument_2_loss: 0.0425 - instrument_3_loss: 0.1961 - instrument_4_loss: 0.2110 - instrument_5_loss: 0.1628 - instrument_6_loss: 0.0737 - instrument_7_loss: 0.0385 - instrument_8_loss: 0.1007 - instrument_9_loss: 0.0145 - loss: 1.1969\n",
      "Epoch 17/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5357s\u001b[0m 4s/step - instrument_10_loss: 0.0253 - instrument_11_loss: 0.0277 - instrument_12_loss: 0.0368 - instrument_13_loss: 0.0349 - instrument_1_loss: 0.2231 - instrument_2_loss: 0.0444 - instrument_3_loss: 0.2049 - instrument_4_loss: 0.2119 - instrument_5_loss: 0.1658 - instrument_6_loss: 0.0805 - instrument_7_loss: 0.0377 - instrument_8_loss: 0.1105 - instrument_9_loss: 0.0146 - loss: 1.2183\n",
      "Epoch 18/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5398s\u001b[0m 4s/step - instrument_10_loss: 0.0241 - instrument_11_loss: 0.0266 - instrument_12_loss: 0.0357 - instrument_13_loss: 0.0337 - instrument_1_loss: 0.2180 - instrument_2_loss: 0.0448 - instrument_3_loss: 0.1919 - instrument_4_loss: 0.2043 - instrument_5_loss: 0.1562 - instrument_6_loss: 0.0790 - instrument_7_loss: 0.0371 - instrument_8_loss: 0.0960 - instrument_9_loss: 0.0132 - loss: 1.1606\n",
      "Epoch 19/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5307s\u001b[0m 4s/step - instrument_10_loss: 0.0262 - instrument_11_loss: 0.0257 - instrument_12_loss: 0.0380 - instrument_13_loss: 0.0340 - instrument_1_loss: 0.2223 - instrument_2_loss: 0.0449 - instrument_3_loss: 0.1982 - instrument_4_loss: 0.2060 - instrument_5_loss: 0.1617 - instrument_6_loss: 0.0793 - instrument_7_loss: 0.0372 - instrument_8_loss: 0.0994 - instrument_9_loss: 0.0156 - loss: 1.1885\n",
      "Epoch 20/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5354s\u001b[0m 4s/step - instrument_10_loss: 0.0250 - instrument_11_loss: 0.0298 - instrument_12_loss: 0.0384 - instrument_13_loss: 0.0345 - instrument_1_loss: 0.2316 - instrument_2_loss: 0.0428 - instrument_3_loss: 0.1946 - instrument_4_loss: 0.2019 - instrument_5_loss: 0.1591 - instrument_6_loss: 0.0750 - instrument_7_loss: 0.0344 - instrument_8_loss: 0.1070 - instrument_9_loss: 0.0150 - loss: 1.1890\n",
      "Epoch 21/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5638s\u001b[0m 4s/step - instrument_10_loss: 0.0236 - instrument_11_loss: 0.0278 - instrument_12_loss: 0.0346 - instrument_13_loss: 0.0337 - instrument_1_loss: 0.2121 - instrument_2_loss: 0.0416 - instrument_3_loss: 0.1970 - instrument_4_loss: 0.2019 - instrument_5_loss: 0.1618 - instrument_6_loss: 0.0737 - instrument_7_loss: 0.0385 - instrument_8_loss: 0.1042 - instrument_9_loss: 0.0148 - loss: 1.1652\n",
      "Epoch 22/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5590s\u001b[0m 4s/step - instrument_10_loss: 0.0256 - instrument_11_loss: 0.0304 - instrument_12_loss: 0.0375 - instrument_13_loss: 0.0368 - instrument_1_loss: 0.2243 - instrument_2_loss: 0.0424 - instrument_3_loss: 0.1975 - instrument_4_loss: 0.1981 - instrument_5_loss: 0.1605 - instrument_6_loss: 0.0748 - instrument_7_loss: 0.0387 - instrument_8_loss: 0.1030 - instrument_9_loss: 0.0143 - loss: 1.1837\n",
      "Epoch 23/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5677s\u001b[0m 4s/step - instrument_10_loss: 0.0254 - instrument_11_loss: 0.0288 - instrument_12_loss: 0.0390 - instrument_13_loss: 0.0340 - instrument_1_loss: 0.2182 - instrument_2_loss: 0.0407 - instrument_3_loss: 0.1936 - instrument_4_loss: 0.1986 - instrument_5_loss: 0.1612 - instrument_6_loss: 0.0808 - instrument_7_loss: 0.0349 - instrument_8_loss: 0.1039 - instrument_9_loss: 0.0147 - loss: 1.1739\n",
      "Epoch 24/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5648s\u001b[0m 4s/step - instrument_10_loss: 0.0242 - instrument_11_loss: 0.0293 - instrument_12_loss: 0.0372 - instrument_13_loss: 0.0343 - instrument_1_loss: 0.2290 - instrument_2_loss: 0.0409 - instrument_3_loss: 0.1945 - instrument_4_loss: 0.2073 - instrument_5_loss: 0.1584 - instrument_6_loss: 0.0775 - instrument_7_loss: 0.0355 - instrument_8_loss: 0.1016 - instrument_9_loss: 0.0139 - loss: 1.1836\n",
      "Epoch 25/25\n",
      "\u001b[1m1275/1275\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5500s\u001b[0m 4s/step - instrument_10_loss: 0.0241 - instrument_11_loss: 0.0286 - instrument_12_loss: 0.0362 - instrument_13_loss: 0.0363 - instrument_1_loss: 0.2236 - instrument_2_loss: 0.0418 - instrument_3_loss: 0.1920 - instrument_4_loss: 0.2012 - instrument_5_loss: 0.1585 - instrument_6_loss: 0.0767 - instrument_7_loss: 0.0358 - instrument_8_loss: 0.1017 - instrument_9_loss: 0.0150 - loss: 1.1717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 20:49:12.242960: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: CANCELLED: GetNextFromShard was cancelled\n",
      "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
      "\t [[RemoteCall]] [type.googleapis.com/tensorflow.DerivedStatus='']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Selected best by loss: ckpt_e23_loss1.173109.weights.h5 (loss=1.173109)\n",
      "âœ… Loaded weights from demucs_v4_fixed_ckpt/ckpt_e23_loss1.173109.weights.h5\n",
      "ðŸ’¾ Saved full model â†’ demucs_v4_fixed_model.keras\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "DATA_ROOT = os.path.expanduser('~/madari3/gcs-bucket/Slakh_Dataset_Chunked/train_chunked')\n",
    "\n",
    "train_demucs_v4_fixed(\n",
    "    data_dir=DATA_ROOT,\n",
    "    batch_size=8,\n",
    "    steps_per_epoch=1275,\n",
    "    epochs=25\n",
    ")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Cloud IDE)",
   "language": "python",
   "name": "cloud-ide"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
