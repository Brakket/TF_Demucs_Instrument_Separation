{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3af7f22e",
      "metadata": {},
      "source": [
        "# Hybrid Demucs V4 Inference\n",
        "\n",
        "This notebook separates a mixed audio file into 13 individual instrument stems using a trained Hybrid Demucs model.\n",
        "\n",
        "## Features\n",
        "- **Overlap-add processing** for seamless separation of long audio files\n",
        "- **Batch prediction** for efficient GPU utilization\n",
        "- **Automatic resampling** to 44.1kHz if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enabled memory growth on 8 GPU(s)\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# GPU Configuration\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable memory growth to avoid allocating all GPU memory upfront\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    print(f'Enabled memory growth on {len(gpus)} GPU(s)')\n",
        "else:\n",
        "    print('No GPUs available - using CPU')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Imports and Constants\n",
        "# ==============================================================================\n",
        "\n",
        "import os, math\n",
        "import numpy as np\n",
        "import librosa, soundfile as sf\n",
        "import tensorflow as tf\n",
        "\n",
        "from demucs_v4_model import (\n",
        "    ExpandDims, ReduceMean, LocalSelfAttention, STFT, InverseSTFT, custom_loss\n",
        ")\n",
        "\n",
        "# Audio parameters\n",
        "SR = 44100          # Sample rate\n",
        "PADDED_LEN = 441_000  # 10 seconds at 44.1kHz\n",
        "\n",
        "# Instrument stem names (13 total)\n",
        "INSTRUMENT_NAMES = [\n",
        "    'Guitar', 'Drums', 'Piano', 'Bass', 'Strings (continued)',\n",
        "    'Organ', 'Synth Lead', 'Synth Pad', 'Chromatic Percussion',\n",
        "    'Brass', 'Pipe', 'Reed', 'Strings'\n",
        "]\n",
        "\n",
        "# Mapping between model output keys and instrument names\n",
        "MODEL_KEYS = {name: f'instrument_{i+1}' for i, name in enumerate(INSTRUMENT_NAMES)}\n",
        "KEY_TO_NAME = {v: k for k, v in MODEL_KEYS.items()}\n",
        "\n",
        "\n",
        "def model_chunk_len(m):\n",
        "    \"\"\"Get the expected input length from model architecture.\"\"\"\n",
        "    s = m.inputs[0].shape\n",
        "    return int(s[1]) if s and s[1] else PADDED_LEN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca16c621",
      "metadata": {},
      "source": [
        "## Load Model\n",
        "\n",
        "Load the trained model with custom layer definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n",
            "Using MirroredStrategy with 8 GPUs\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ListWrapper(['conv1d_155', 'conv1d_173', 'conv1d_175', 'conv1d_177', 'conv1d_179', 'conv1d_157', 'conv1d_159', 'conv1d_161', 'conv1d_163', 'conv1d_165', 'conv1d_167', 'conv1d_169', 'conv1d_171'])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load pre-trained model with custom layer definitions\n",
        "model = tf.keras.models.load_model(\n",
        "    'demucs_v4_fixed_model.keras',\n",
        "    custom_objects={\n",
        "        'ExpandDims': ExpandDims,\n",
        "        'ReduceMean': ReduceMean,\n",
        "        'LocalSelfAttention': LocalSelfAttention,\n",
        "        'STFT': STFT,\n",
        "        'InverseSTFT': InverseSTFT,\n",
        "        'custom_loss': custom_loss,\n",
        "    },\n",
        "    compile=False,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded with {len(model.output_names)} output stems\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c0d2348",
      "metadata": {},
      "source": [
        "## Separation Function\n",
        "\n",
        "The main audio separation function with overlap-add processing for seamless output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def separate_long_audio(model, audio_path, output_dir, sr=SR, batch_size=8):\n",
        "    \"\"\"\n",
        "    Separate a full-length audio file into instrument stems.\n",
        "    \n",
        "    Uses overlap-add processing with batched prediction for efficient GPU utilization.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained Keras model\n",
        "        audio_path: Path to input audio file\n",
        "        output_dir: Directory to save separated stems\n",
        "        sr: Sample rate (default: 44100)\n",
        "        batch_size: Number of chunks to process in parallel\n",
        "    \"\"\"\n",
        "    audio_path = os.path.expanduser(audio_path)\n",
        "    output_dir = os.path.expanduser(output_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load and preprocess audio\n",
        "    wav, file_sr = sf.read(audio_path, always_2d=False)\n",
        "    if file_sr != sr:\n",
        "        wav = librosa.resample(wav, orig_sr=file_sr, target_sr=sr)\n",
        "    if wav.ndim == 2:\n",
        "        wav = wav.mean(axis=1)  # Convert stereo to mono\n",
        "    wav = wav.astype(np.float32)\n",
        "\n",
        "    # Processing parameters\n",
        "    CHUNK = model_chunk_len(model)\n",
        "    HOP = CHUNK // 2  # 50% overlap\n",
        "    WIN = np.hanning(CHUNK).astype(np.float32)\n",
        "    N = len(wav)\n",
        "\n",
        "    # Probe first chunk to determine output structure\n",
        "    head = wav[:CHUNK]\n",
        "    peak0 = float(np.max(np.abs(head))) or 1.0\n",
        "    if len(head) < CHUNK:\n",
        "        head = np.pad(head, (0, CHUNK - len(head)))\n",
        "    head_in = (head / peak0).astype(np.float32)[np.newaxis, :, np.newaxis]\n",
        "    test_out = model.predict(head_in, verbose=0)\n",
        "    \n",
        "    if isinstance(test_out, dict):\n",
        "        keys = list(test_out.keys())\n",
        "        use_dict = True\n",
        "    else:\n",
        "        keys = list(range(len(test_out)))\n",
        "        use_dict = False\n",
        "\n",
        "    # Initialize overlap-add accumulators\n",
        "    if use_dict:\n",
        "        acc = {k: np.zeros(N + CHUNK, np.float32) for k in keys}\n",
        "    else:\n",
        "        acc = [np.zeros(N + CHUNK, np.float32) for _ in keys]\n",
        "    wsum = np.zeros(N + CHUNK, np.float32)\n",
        "\n",
        "    # Batch processing buffers\n",
        "    batch = []\n",
        "    starts = []\n",
        "\n",
        "    def flush():\n",
        "        \"\"\"Process accumulated batch and add to accumulators.\"\"\"\n",
        "        if not batch:\n",
        "            return\n",
        "        xin = np.stack(batch, axis=0)\n",
        "        out = model.predict(xin, verbose=0)\n",
        "        for b, (start, peak) in enumerate(starts):\n",
        "            if use_dict:\n",
        "                for k in keys:\n",
        "                    y = out[k][b, :, 0].astype(np.float32) * peak\n",
        "                    acc[k][start:start+CHUNK] += y * WIN\n",
        "            else:\n",
        "                for i in keys:\n",
        "                    y = out[i][b, :, 0].astype(np.float32) * peak\n",
        "                    acc[i][start:start+CHUNK] += y * WIN\n",
        "            wsum[start:start+CHUNK] += WIN\n",
        "        batch.clear()\n",
        "        starts.clear()\n",
        "\n",
        "    # Process audio in overlapping chunks\n",
        "    pos = 0\n",
        "    while pos < N:\n",
        "        end = min(pos + CHUNK, N)\n",
        "        x = wav[pos:end]\n",
        "        peak = float(np.max(np.abs(x))) if len(x) else 1.0\n",
        "        if peak < 1e-7:\n",
        "            peak = 1.0\n",
        "        if len(x) < CHUNK:\n",
        "            x = np.pad(x, (0, CHUNK - len(x)))\n",
        "        xin = (x / peak).astype(np.float32)[..., None]\n",
        "        batch.append(xin)\n",
        "        starts.append((pos, peak))\n",
        "        if len(batch) == batch_size:\n",
        "            flush()\n",
        "        pos += HOP\n",
        "    flush()\n",
        "\n",
        "    # Normalize and save stems\n",
        "    eps = 1e-8\n",
        "    if use_dict:\n",
        "        for k in keys:\n",
        "            acc[k][:N] = acc[k][:N] / np.maximum(wsum[:N], eps)\n",
        "            name = KEY_TO_NAME.get(k, str(k))\n",
        "            sf.write(os.path.join(output_dir, f'{name}.wav'), \n",
        "                    acc[k][:N].astype(np.float32), sr, subtype='PCM_16')\n",
        "    else:\n",
        "        for i in keys:\n",
        "            y = (acc[i][:N] / np.maximum(wsum[:N], eps)).astype(np.float32)\n",
        "            name = INSTRUMENT_NAMES[i] if i < len(INSTRUMENT_NAMES) else f\"stem_{i+1}\"\n",
        "            sf.write(os.path.join(output_dir, f'{name}.wav'), y, sr, subtype='PCM_16')\n",
        "    \n",
        "    print(f\"Saved {len(keys)} stems to {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5663dd9b",
      "metadata": {},
      "source": [
        "## Run Separation\n",
        "\n",
        "Separate an audio file into stems. Adjust paths and batch size as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-30 04:29:48.579940: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\n",
            "\t [[RemoteCall]]\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'conv1d_155'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_26604/1452345236.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Uncomment to run:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mseparate_long_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAUDIO_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_26604/2669666678.py\u001b[0m in \u001b[0;36mseparate_long_audio\u001b[0;34m(model, audio_path, output_dir, sr, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mstarts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeak\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mHOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_26604/2669666678.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeak\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpeak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mCHUNK\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mWIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mwsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mCHUNK\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mWIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'conv1d_155'"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "AUDIO_PATH = '~/path/to/input.wav'   # Input audio file\n",
        "OUTPUT_DIR = '~/path/to/output/'     # Output directory for stems\n",
        "BATCH_SIZE = 8                        # Increase for more GPU memory usage\n",
        "\n",
        "# Run separation\n",
        "separate_long_audio(model, AUDIO_PATH, OUTPUT_DIR, batch_size=BATCH_SIZE)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Cloud IDE)",
      "language": "python",
      "name": "cloud-ide"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
